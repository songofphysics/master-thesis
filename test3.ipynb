{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt\n",
    "import qutip as qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import json\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-23 02:06:28.926983: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-23 02:06:28.940794: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-23 02:06:28.955577: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-23 02:06:28.960090: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-23 02:06:28.970605: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-23 02:06:29.958857: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729641991.841639 3013308 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729641991.847341 3013308 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729641991.847477 3013308 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729641991.849655 3013308 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729641991.849759 3013308 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729641991.849828 3013308 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729641991.923722 3013308 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729641991.923863 3013308 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1729641991.923947 3013308 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-23 02:06:31.924020: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 172 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "# Enable GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.FATAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def get_vacuum_state_tf(dim):\n",
    "    vacuum_state = tf.zeros([dim, 1], dtype=tf.complex64)\n",
    "    one = tf.constant([[1.0 + 0.0j]], dtype=tf.complex64)\n",
    "    vacuum_state = tf.concat([one, vacuum_state[1:]], axis=0)\n",
    "    return vacuum_state\n",
    "\n",
    "@tf.function\n",
    "def annihilation(dim):\n",
    "    diag_vals = tf.sqrt(tf.cast(tf.range(1, dim), dtype=tf.complex64))\n",
    "    return tf.linalg.diag(diag_vals, k=1)\n",
    "\n",
    "@tf.function\n",
    "def number(dim):\n",
    "    return tf.linalg.diag(tf.cast(tf.range(dim), dtype=tf.complex64))\n",
    "\n",
    "@tf.function\n",
    "def displacement_operator(dim, x, y=0):\n",
    "    alpha = tf.complex(x, y)\n",
    "    a = annihilation(dim)\n",
    "    a_dag = tf.linalg.adjoint(a)\n",
    "    exponent = alpha * a_dag - tf.math.conj(alpha) * a\n",
    "    return tf.linalg.expm(exponent)\n",
    "\n",
    "@tf.function\n",
    "def displacement_encoding(dim, alpha_vec):\n",
    "    alpha_vec = tf.cast(alpha_vec, dtype=tf.complex64)\n",
    "    num = tf.shape(alpha_vec)[0]\n",
    "    a = annihilation(dim)\n",
    "    a_dag = tf.linalg.adjoint(a)\n",
    "    \n",
    "    alpha_vec = tf.reshape(alpha_vec, [-1, 1, 1])\n",
    "    exponent = alpha_vec * tf.expand_dims(a_dag, 0) - tf.math.conj(alpha_vec) * tf.expand_dims(a, 0)\n",
    "    return tf.linalg.expm(exponent)\n",
    "\n",
    "@tf.function\n",
    "def rotation_operator(dim, theta):\n",
    "    theta = tf.cast(theta, dtype=tf.complex64)\n",
    "    n = number(dim)\n",
    "    return tf.linalg.expm(-1j * theta * n)\n",
    "\n",
    "@tf.function\n",
    "def squeezing_operator(dim, r):\n",
    "    r = tf.cast(r, dtype=tf.complex64)\n",
    "    a = annihilation(dim)\n",
    "    a_dag = tf.linalg.adjoint(a)\n",
    "    exponent = 0.5 * (tf.math.conj(r) * (a @ a) - r * (a_dag @ a_dag))\n",
    "    return tf.linalg.expm(exponent)\n",
    "\n",
    "@tf.function\n",
    "def kerr_operator(dim, kappa):\n",
    "    kappa = tf.cast(kappa, dtype=tf.complex64)\n",
    "    n = number(dim)\n",
    "    return tf.linalg.expm(1j * kappa * (n @ n))\n",
    "\n",
    "@tf.function\n",
    "def cubic_phase_operator(dim, gamma):\n",
    "    gamma = tf.cast(gamma, dtype=tf.complex64)\n",
    "    a = annihilation(dim)\n",
    "    x = (a + tf.linalg.adjoint(a)) / tf.cast(2.0, dtype=tf.complex64)\n",
    "    return tf.linalg.expm(1j * (gamma/3) * (x @ x @ x))\n",
    "\n",
    "@tf.function\n",
    "def categorically_sample(probs, num_samples):\n",
    "    # Generate uniform random numbers\n",
    "    uniform_samples = tf.random.uniform(shape=(num_samples, tf.shape(probs)[0], 1))\n",
    "    \n",
    "    # Compute cumulative probabilities\n",
    "    cumulative_probs = tf.cumsum(probs, axis=1)\n",
    "    \n",
    "    # Expand dimensions for broadcasting\n",
    "    cumulative_probs = tf.expand_dims(cumulative_probs, axis=0)\n",
    "    \n",
    "    # Compare uniform samples with cumulative probabilities\n",
    "    samples = tf.reduce_sum(tf.cast(uniform_samples > cumulative_probs, tf.int32), axis=2)\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow Custom Layer for Quantum Encoding\n",
    "class QEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, vacuum_state, **kwargs):\n",
    "        super(QEncoder, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.vacuum_state = tf.cast(vacuum_state, dtype=tf.complex64)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        squeezed_vacuum_state = tf.matmul(squeezing_operator(self.dim, 2.0), self.vacuum_state)\n",
    "        batch_squeezed_state = tf.tile(tf.expand_dims(squeezed_vacuum_state, axis=0), [batch_size, 1, 1])\n",
    "        batch_displacement_operators = displacement_encoding(self.dim, inputs/2)\n",
    "        displaced_states = tf.einsum('bij,bjk->bik', batch_displacement_operators, batch_squeezed_state)\n",
    "        \n",
    "        # Normalize the states\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.abs(displaced_states)**2, axis=1, keepdims=True))\n",
    "        norm = tf.cast(norm, dtype=tf.complex64)\n",
    "        normalized_states = displaced_states / norm\n",
    "        \n",
    "        # Convert to density matrices\n",
    "        density_matrices = tf.einsum('bij,bkj->bik', normalized_states, tf.math.conj(normalized_states))\n",
    "        \n",
    "        return density_matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, stddev=0.05, activation='kerr', **kwargs):\n",
    "        super(QLayer, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.stddev = stddev\n",
    "        self.activation = activation.lower()\n",
    "        if self.activation not in ['kerr', 'cubicphase']:\n",
    "            raise ValueError(\"Activation must be either 'kerr' or 'cubicphase'\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        initializer = tf.keras.initializers.RandomNormal(mean=0.0, stddev=self.stddev, seed=42)\n",
    "        self.theta_1 = self.add_weight(name=\"theta_1\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.theta_2 = self.add_weight(name=\"theta_2\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.r = self.add_weight(name=\"r\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.bx = self.add_weight(name=\"bx\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        self.bp = self.add_weight(name=\"bp\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        \n",
    "        if self.activation == 'kerr':\n",
    "            self.kappa = self.add_weight(name=\"kappa\", shape=[1,], initializer=initializer, trainable=True)\n",
    "        else:  # cubicphase\n",
    "            self.gamma = self.add_weight(name=\"gamma\", shape=[1,], initializer=initializer, trainable=True)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        # Create operators\n",
    "        R_1 = rotation_operator(self.dim, self.theta_1)\n",
    "        S = squeezing_operator(self.dim, self.r)\n",
    "        R_2 = rotation_operator(self.dim, self.theta_2)\n",
    "        D = displacement_operator(self.dim, self.bx, self.bp)\n",
    "        \n",
    "        if self.activation == 'kerr':\n",
    "            A = kerr_operator(self.dim, self.kappa)\n",
    "        else:  # cubicphase\n",
    "            A = cubic_phase_operator(self.dim, self.gamma)\n",
    "\n",
    "        # Combine all operators into a single unitary\n",
    "        U = A@D@R_2@S@R_1\n",
    "        \n",
    "        # Expand U to match batch size\n",
    "        U_batch = tf.tile(tf.expand_dims(U, 0), [batch_size, 1, 1])\n",
    "        \n",
    "        # Apply the combined operation\n",
    "        return tf.einsum('bij,bjk,blk->bil', U_batch, inputs, tf.math.conj(U_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossChannel(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, T=1.0, **kwargs):\n",
    "        super(LossChannel, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.T = T\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.a = self.add_weight(\n",
    "            name='annihilation_operator',\n",
    "            shape=(self.dim, self.dim),\n",
    "            initializer=lambda shape, dtype: annihilation(self.dim),\n",
    "            trainable=False,\n",
    "            dtype=tf.complex64\n",
    "        )\n",
    "        self.factor = tf.cast((1 - self.T) / self.T, dtype=tf.complex64)\n",
    "        self.sqrt_T = tf.cast(tf.sqrt(self.T), dtype=tf.complex64)\n",
    "        self.a_dag_a = tf.matmul(self.a, self.a, adjoint_a=True)\n",
    "        self.sqrt_T_pow_a_dag_a = tf.linalg.expm(tf.math.log(self.sqrt_T) * self.a_dag_a)\n",
    "        \n",
    "        # Use int32 for range, then cast to complex64 for calculations\n",
    "        n_range = tf.cast(tf.range(self.dim), dtype=tf.complex64)\n",
    "        factorial_term = tf.exp(tf.math.lgamma(tf.cast(n_range + 1, dtype=tf.float32)))\n",
    "        power_term = tf.exp(n_range / 2 * tf.math.log(self.factor))  # Using exp(log()) instead of pow()\n",
    "        self.E_n_diag = power_term / tf.sqrt(tf.cast(factorial_term, dtype=tf.complex64))\n",
    "        \n",
    "        # Create a_powers using complex calculations\n",
    "        a_expanded = tf.reshape(self.a, (self.dim, self.dim, 1))\n",
    "        n_expanded = tf.reshape(n_range, (1, 1, self.dim))\n",
    "        eye = tf.eye(self.dim, dtype=tf.complex64)[:, :, tf.newaxis]\n",
    "        self.a_powers = tf.exp(n_expanded * tf.math.log(tf.einsum('ijk,lik->ijl', a_expanded, eye)))\n",
    "\n",
    "        super(LossChannel, self).build(input_shape)\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        E_n_diag_expanded = self.E_n_diag[:, tf.newaxis, tf.newaxis]\n",
    "        E_n = E_n_diag_expanded * self.a_powers\n",
    "        E_n = tf.matmul(E_n, self.sqrt_T_pow_a_dag_a)\n",
    "        E_n_expanded = tf.repeat(E_n[tf.newaxis, :, :, :], batch_size, axis=0)\n",
    "        inputs_expanded = inputs[:, tf.newaxis, :, :]\n",
    "        E_n_rho = tf.matmul(E_n_expanded, inputs_expanded)\n",
    "        E_n_rho_E_n_dag = tf.matmul(E_n_rho, E_n_expanded, adjoint_b=True)\n",
    "        N_T_rho = tf.reduce_sum(E_n_rho_E_n_dag, axis=1)\n",
    "        return N_T_rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, num_measurements=1000, sampling=False, **kwargs):\n",
    "        super(QDecoder, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.sampling = sampling\n",
    "        self.num_measurements = num_measurements\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.sampling:\n",
    "            x_operator = self.build_x_operator()\n",
    "            eigenvalues, eigenvectors = tf.linalg.eigh(x_operator)\n",
    "            \n",
    "            self.eigenvalues = self.add_weight(\n",
    "                name='eigenvalues',\n",
    "                shape=eigenvalues.shape,\n",
    "                dtype=tf.complex64,\n",
    "                initializer=lambda shape, dtype: eigenvalues,\n",
    "                trainable=False\n",
    "            )\n",
    "            self.eigenvectors = self.add_weight(\n",
    "                name='eigenvectors',\n",
    "                shape=eigenvectors.shape,\n",
    "                dtype=tf.complex64,\n",
    "                initializer=lambda shape, dtype: eigenvectors,\n",
    "                trainable=False\n",
    "            )\n",
    "        else:\n",
    "            x_operator = self.build_x_operator()\n",
    "            self.x_quad = self.add_weight(\n",
    "                name='x_quad',\n",
    "                shape=x_operator.shape,\n",
    "                dtype=tf.complex64,\n",
    "                initializer=lambda shape, dtype: x_operator,\n",
    "                trainable=False\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def build_x_operator(self):\n",
    "        a = annihilation(self.dim)\n",
    "        x_operator = (a + tf.linalg.adjoint(a)) / 2.0\n",
    "        return x_operator\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "\n",
    "        if self.sampling:\n",
    "            probabilities = tf.einsum('ji,njk,ki->ni', \n",
    "                            tf.math.conj(self.eigenvectors),\n",
    "                            inputs,\n",
    "                            self.eigenvectors)\n",
    "            \n",
    "            probabilities = tf.math.real(probabilities)\n",
    "            probabilities = probabilities / tf.reduce_sum(probabilities, axis=-1, keepdims=True)\n",
    "            eigenvalues = tf.math.real(self.eigenvalues)\n",
    "\n",
    "            indices = tf.raw_ops.Multinomial(logits=tf.math.log(probabilities), \n",
    "                                             num_samples=self.num_measurements)\n",
    "            samples = tf.gather(eigenvalues, indices)\n",
    "            averages = tf.reduce_mean(samples, axis=1, keepdims=True)\n",
    "            \n",
    "            return averages\n",
    "        else:\n",
    "            outcomes = tf.einsum('njk,jk->n', inputs, self.x_quad)\n",
    "            return tf.expand_dims(tf.math.real(outcomes), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.3704537  -0.39212987 -0.36144742 -0.416209   -0.35741785 -0.3944429\n",
      " -0.41665706 -0.46226305 -0.45743996 -0.4550311  -0.5325401  -0.4033587\n",
      " -0.48351955 -0.53891635 -0.5740504  -0.6030654  -0.49986285 -0.4940953\n",
      " -0.5242941  -0.53881454 -0.4995661  -0.517824   -0.5792964  -0.5078233\n",
      " -0.5129964  -0.5434817  -0.5170561  -0.54346406 -0.43815348 -0.5205842\n",
      " -0.43444052 -0.35815218 -0.55093706 -0.44152394 -0.40779924 -0.36774245\n",
      " -0.29113954 -0.33096814 -0.34693384 -0.40571642 -0.30460364 -0.35264626\n",
      " -0.34941044 -0.29985505 -0.316235   -0.26818508 -0.27233297 -0.24184929\n",
      " -0.23551548 -0.27782154 -0.14868146 -0.27381775 -0.19764636 -0.18879905\n",
      " -0.19174008 -0.22652806 -0.20281552 -0.13961723 -0.16998255 -0.18045382\n",
      " -0.082986   -0.13604026 -0.07361823 -0.05035209 -0.0626412  -0.1035062\n",
      " -0.22082719 -0.13438335 -0.10607465 -0.09089949 -0.08085649 -0.12489627\n",
      " -0.12393805 -0.14787711 -0.14618091 -0.15410237 -0.13478012 -0.21807188\n",
      " -0.13561668 -0.06118298 -0.11204639 -0.12384138 -0.1922704  -0.11631334\n",
      " -0.15212491 -0.18602733 -0.17793082 -0.09218692 -0.05608297 -0.13920243\n",
      " -0.1776169  -0.15581843 -0.12668444 -0.12367398 -0.0683518  -0.08591312\n",
      " -0.00614993 -0.01385404 -0.00956111 -0.03860028  0.03466474  0.04567137\n",
      "  0.06122206  0.14375709  0.07795217  0.18215267  0.0516637   0.09848333\n",
      "  0.06843229  0.11303274  0.06179057  0.11563695  0.16937312  0.12625273\n",
      "  0.11209533  0.10730843  0.19748232  0.1350016   0.17096122  0.17371412\n",
      "  0.12594701  0.14054328  0.14877033  0.20160846  0.19222854  0.19411114\n",
      "  0.23044352  0.12669872  0.14479949  0.14271192  0.10084199  0.09241752\n",
      "  0.08979047  0.1402061   0.17460908  0.09871917  0.17423236  0.1405719\n",
      "  0.15054329  0.11576039  0.10506169  0.22107224  0.17549798  0.15714905\n",
      "  0.1545927   0.10592928  0.238761    0.1949636   0.21315794  0.18233973\n",
      "  0.21646607  0.2702963   0.16915525  0.3188168   0.2558015   0.32382348\n",
      "  0.32967234  0.31828663  0.22177012  0.3211659   0.26615992  0.4017686\n",
      "  0.47161934  0.42635205  0.3422771   0.46884602  0.42752412  0.41553125\n",
      "  0.46551436  0.39470625  0.4553079   0.5566934   0.49546415  0.55067706\n",
      "  0.6370284   0.5315555   0.48783964  0.47333717  0.5752714   0.50178695\n",
      "  0.58021307  0.5592746   0.5112771   0.6265575   0.5320635   0.5498365\n",
      "  0.54595935  0.47639924  0.48772305  0.4314523   0.52799624  0.52017546\n",
      "  0.44704738  0.40479043  0.44993427  0.43679878  0.4220133   0.34048006\n",
      "  0.41038004  0.3454966 ], shape=(200,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "dim = 10\n",
    "num_measurements = 1000\n",
    "num_bins = 200\n",
    "x_values = np.linspace(-np.pi, np.pi, num_bins).reshape(-1, 1)\n",
    "\n",
    "qenc = QEncoder(dim, get_vacuum_state_tf(dim))\n",
    "layer = QLayer(dim)\n",
    "states = layer(qenc(x_values))\n",
    "\n",
    "a = annihilation(dim)\n",
    "x_operator = (a + tf.linalg.adjoint(a)) / 2.0\n",
    "x, xvec = tf.linalg.eigh(x_operator)\n",
    "\n",
    "probabilities = tf.einsum('ji,njk,ki->ni', tf.math.conj(xvec), states, xvec)\n",
    "    \n",
    "probabilities = tf.math.real(probabilities)\n",
    "\n",
    "indices = tf.random.categorical(tf.math.log(probabilities), num_measurements)\n",
    "# print(indices)\n",
    "samples = tf.gather(x, indices)\n",
    "averages = tf.reduce_mean(samples, axis=1, keepdims=True)\n",
    "\n",
    "print(tf.squeeze(tf.math.real(averages)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[2 1 1 ... 2 2 2]\n",
      " [2 1 1 ... 2 2 2]\n",
      " [2 1 1 ... 2 2 2]\n",
      " ...\n",
      " [8 3 5 ... 7 8 5]\n",
      " [8 3 5 ... 7 8 5]\n",
      " [8 3 5 ... 7 8 5]], shape=(200, 1000), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# Gumbel-Max sampling\n",
    "gumbel_noise = -np.log(-np.log(np.random.uniform(size=(num_measurements, dim))))\n",
    "gumbel_noise = tf.constant(gumbel_noise, dtype=tf.float32)\n",
    "noisy_logits = tf.expand_dims(tf.math.log(probabilities + 1e-10), 1) + gumbel_noise\n",
    "indices_gumbel = tf.argmax(noisy_logits, axis=-1)\n",
    "print(indices_gumbel)\n",
    "samples_gumbel = tf.gather(x, indices_gumbel)\n",
    "averages_gumbel = tf.reduce_mean(samples_gumbel, axis=1)\n",
    "\n",
    "# print(tf.squeeze(tf.math.real(averages_gumbel)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 4\n",
    "inps = np.linspace(-1.,1.,10).reshape(-1,1)\n",
    "print(inps.shape)\n",
    "qenc = QEncoder(dim, get_vacuum_state_tf(dim))\n",
    "encs = qenc(inps)\n",
    "print(encs.shape)\n",
    "\n",
    "layer = QLayer(dim)\n",
    "outs = layer(encs)\n",
    "print(outs.shape)\n",
    "\n",
    "loss = LossChannel(dim)\n",
    "ends = loss(outs)\n",
    "print(ends.shape)\n",
    "\n",
    "meas = QDecoder(dim, sampling=False)\n",
    "pred = meas(ends)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2Score metric wrapper to handle shape issues\n",
    "class R2ScoreWrapper(tf.keras.metrics.R2Score):\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        y_true = tf.reshape(y_true, [-1, 1])\n",
    "        y_pred = tf.reshape(y_pred, [-1, 1])\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n",
    "    \n",
    "\n",
    "# TensorFlow Custom Callback for Progress Bars\n",
    "from IPython.display import clear_output\n",
    "\n",
    "class TrainingProgress(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        epoch += 1  # epochs are zero-indexed in this method\n",
    "        \n",
    "        # Get training loss, validation loss, and learning rate\n",
    "        train_loss = logs.get('loss')\n",
    "        val_loss = logs.get('val_loss')\n",
    "        lr = self.model.optimizer.learning_rate\n",
    "        \n",
    "        # If lr is a callable (LearningRateSchedule), get its current value\n",
    "        if callable(lr):\n",
    "            lr = lr(self.model.optimizer.iterations)\n",
    "        \n",
    "        # Convert lr tensor to float\n",
    "        lr = float(lr)\n",
    "\n",
    "        print(f\"Epoch: {epoch:5d} | LR: {lr:.7f} | Loss: {train_loss:.7f} | Val Loss: {val_loss:.7f}\")\n",
    "\n",
    "        # Every 5 epochs, clear the screen\n",
    "        if epoch % 5 == 0:\n",
    "            clear_output(wait=True)\n",
    "\n",
    "# TensorFlow Custom Callback for Parameter Logging\n",
    "class ParameterLoggingCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, fold, function_index, activation, base_dir='Params'):\n",
    "        super(ParameterLoggingCallback, self).__init__()\n",
    "        self.fold = fold\n",
    "        self.function_index = function_index\n",
    "        self.activation = activation\n",
    "        self.base_dir = base_dir\n",
    "        self.params_dir = os.path.join(base_dir, f'Function_{function_index}')\n",
    "        self.filename = os.path.join(self.params_dir, f'parameters_fold_{fold}.csv')\n",
    "        self.epoch = 0\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(self.params_dir, exist_ok=True)\n",
    "        \n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Count the number of QLayers\n",
    "        self.num_qlayers = sum(1 for layer in self.model.layers if isinstance(layer, QLayer))\n",
    "        \n",
    "        # Create the CSV file for parameters and write the header\n",
    "        with open(self.filename, 'w', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            header = ['Epoch']\n",
    "            for i in range(self.num_qlayers):\n",
    "                if self.activation == 'kerr':\n",
    "                    header.extend([f'Layer{i}_theta_1', f'Layer{i}_r', f'Layer{i}_theta_2', \n",
    "                               f'Layer{i}_bx', f'Layer{i}_bp', f'Layer{i}_kappa'])\n",
    "                else:\n",
    "                    header.extend([f'Layer{i}_theta_1', f'Layer{i}_r', f'Layer{i}_theta_2', \n",
    "                               f'Layer{i}_bx', f'Layer{i}_bp', f'Layer{i}_gamma'])\n",
    "            writer.writerow(header)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.epoch += 1\n",
    "        params = []\n",
    "        for layer in self.model.layers:\n",
    "            if isinstance(layer, QLayer):\n",
    "                if self.activation == 'kerr':\n",
    "                    layer_params = [\n",
    "                        layer.theta_1.numpy()[0],\n",
    "                        layer.r.numpy()[0],\n",
    "                        layer.theta_2.numpy()[0],\n",
    "                        layer.bx.numpy()[0],\n",
    "                        layer.bp.numpy()[0],\n",
    "                        layer.kappa.numpy()[0]\n",
    "                    ]\n",
    "                else:\n",
    "                    layer_params = [\n",
    "                        layer.theta_1.numpy()[0],\n",
    "                        layer.r.numpy()[0],\n",
    "                        layer.theta_2.numpy()[0],\n",
    "                        layer.bx.numpy()[0],\n",
    "                        layer.bp.numpy()[0],\n",
    "                        layer.gamma.numpy()[0]\n",
    "                    ]    \n",
    "                params.extend(layer_params)\n",
    "        \n",
    "        # Append the parameters to the CSV file\n",
    "        with open(self.filename, 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([self.epoch] + params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x, eps=0.0):\n",
    "    \"\"\"The function f(x)= |x| + noise\"\"\"\n",
    "    return np.abs(x) + eps * np.random.normal(size=x.shape)\n",
    "\n",
    "\n",
    "def f2(x, eps=0.0):\n",
    "    \"\"\"The function f(x)=sin(x) + noise\"\"\"\n",
    "    return np.sin(x) + eps * np.random.normal(size=x.shape)\n",
    "\n",
    "\n",
    "def f3(x, eps=0.0):\n",
    "    \"\"\"The function f(x)=exp(x)+noise\"\"\"\n",
    "    return np.exp(x) + eps * np.random.normal(size=x.shape)\n",
    "\n",
    "\n",
    "def f4(x, eps=0.0):\n",
    "    \"\"\"The function f(x)=log(x+2*pi)+noise\"\"\"\n",
    "    return np.log(x+2*np.pi) + eps * np.random.normal(size=x.shape)\n",
    "\n",
    "\n",
    "def f5(x, eps=0.0):\n",
    "    \"\"\"The function f(x)=x^3+noise\"\"\"\n",
    "    return x**3 + eps * np.random.normal(size=x.shape)\n",
    "\n",
    "def f6(x, eps=0.0):\n",
    "    \"\"\"The function f(x)=x^3+noise\"\"\"\n",
    "    return np.exp(-(x**2)/2) + eps * np.random.normal(size=x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare your dataset\n",
    "x = np.linspace(-np.pi, np.pi, 200).reshape(-1, 1)\n",
    "F = [f1, f2, f3, f4, f5, f6]\n",
    "Y = np.array([f(x, eps=0.1) for f in F])\n",
    "\n",
    "def min_max_normalize(data):\n",
    "    min_val = data.min(axis=1, keepdims=True)\n",
    "    max_val = data.max(axis=1, keepdims=True)\n",
    "    return (data - min_val) / (max_val - min_val)\n",
    "\n",
    "# Normalize each result individually\n",
    "Y = min_max_normalize(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def train_model(input_data, target_data, function_index, k_folds=5, learning_rate=0.01, std=0.05, cutoff_dim=10, num_layers=2, epochs=200, non_gaussian='kerr', rec=True):\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    print(f'Training model for Function {function_index} with {num_layers} layers for {epochs} epochs...')\n",
    "    \n",
    "    fold_histories = []\n",
    "    models = []\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(input_data), 1):\n",
    "        print(f'Training on fold {fold}...')\n",
    "        \n",
    "        x_train_fold, x_val_fold = input_data[train_index], input_data[val_index]\n",
    "        y_train_fold, y_val_fold = target_data[train_index], target_data[val_index]\n",
    "\n",
    "        model = create_model(cutoff_dim, num_layers, non_gaussian, std)\n",
    "        opt = Adam(learning_rate=learning_rate, clipnorm=1.0)\n",
    "        model.compile(optimizer=opt, loss='mse', metrics=[R2ScoreWrapper()])\n",
    "        \n",
    "        callbacks = [TrainingProgress()]\n",
    "        if rec:\n",
    "            callbacks.append(ParameterLoggingCallback(fold, function_index))\n",
    "        \n",
    "        history = model.fit(x_train_fold, y_train_fold, validation_data=(x_val_fold, y_val_fold), \n",
    "                            epochs=epochs, verbose=0, callbacks=callbacks)\n",
    "        \n",
    "        fold_histories.append(history.history)\n",
    "        models.append(model)\n",
    "        \n",
    "        print(f'Fold {fold} complete.')\n",
    "\n",
    "    # Calculate average cross-validated histories\n",
    "    avg_history = {key: np.mean([h[key] for h in fold_histories], axis=0) for key in fold_histories[0].keys()}\n",
    "    \n",
    "    # Find the best model based on final validation loss\n",
    "    best_model_index = np.argmin([h['val_loss'][-1] for h in fold_histories])\n",
    "    best_model = models[best_model_index]\n",
    "\n",
    "    print('Cross-validation complete.')\n",
    "    print(f'Best model from fold {best_model_index + 1}')\n",
    "    best_model.summary()\n",
    "\n",
    "    return avg_history, best_model\n",
    "\n",
    "def create_model(cutoff_dim, num_layers, non_gaussian, std):\n",
    "    vacuum_state = get_vacuum_state_tf(cutoff_dim)\n",
    "    model = tf.keras.Sequential([QEncoder(dim=cutoff_dim, vacuum_state=vacuum_state, name='QuantumEncoding')])\n",
    "    for i in range(num_layers):\n",
    "        model.add(QLayer(dim=cutoff_dim, activation=non_gaussian, stddev=std, name=f'QuantumLayer_{i+1}'))\n",
    "        #model.add(LossChannel(dim=cutoff_dim, name=f'LossChannel_{i+1}'))\n",
    "    model.add(QDecoder(dim=cutoff_dim, sampling=True, name='QuantumDecoding'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for Function 1 with 6 layers for 50 epochs...\n",
      "Training on fold 1...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m layers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m6\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m6\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m6\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_gaussian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkerr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m H, M \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(input_data, target_data, function_index, k_folds, learning_rate, std, cutoff_dim, num_layers, epochs, non_gaussian, rec)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rec:\n\u001b[1;32m     24\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(ParameterLoggingCallback(fold, function_index))\n\u001b[0;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_val_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val_fold\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m fold_histories\u001b[38;5;241m.\u001b[39mappend(history\u001b[38;5;241m.\u001b[39mhistory)\n\u001b[1;32m     30\u001b[0m models\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "File \u001b[0;32m/scratch/Song/tfenv/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/scratch/Song/tfenv/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:729\u001b[0m, in \u001b[0;36mBaseOptimizer._filter_empty_gradients\u001b[0;34m(self, grads, vars)\u001b[0m\n\u001b[1;32m    726\u001b[0m         missing_grad_vars\u001b[38;5;241m.\u001b[39mappend(v\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filtered_grads:\n\u001b[0;32m--> 729\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo gradients provided for any variable.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_grad_vars:\n\u001b[1;32m    731\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    732\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGradients do not exist for variables \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    733\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mreversed\u001b[39m(missing_grad_vars))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m when minimizing the loss.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    734\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m If using `model.compile()`, did you forget to provide a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    735\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`loss` argument?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    736\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable."
     ]
    }
   ],
   "source": [
    "# tf.data.experimental.enable_debug_mode()\n",
    "results = []\n",
    "layers = [6]*6\n",
    "for i in range(6):\n",
    "    results.append(train_model(x, Y[i], i+1, num_layers=layers[i], cutoff_dim=10, epochs=50, non_gaussian='kerr', rec=False))\n",
    "H, M = zip(*results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
